{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f2b56d",
   "metadata": {},
   "source": [
    "# Bipedal Walker - PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b633067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant functions\n",
    "\n",
    "import gym \n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2bc5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Applies moving average based on a time window\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')\n",
    "\n",
    "\n",
    "def plot_results(log_folder, title='Learning Curve'):\n",
    "    \"\"\"\n",
    "    Plots results of training model\n",
    "    \"\"\"\n",
    "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
    "\n",
    "    # Plotting results before applying moving average\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    # Applying moving average function\n",
    "    y = moving_average(y, window=50)\n",
    "    \n",
    "    # Truncate x\n",
    "    x = x[len(x) - len(y):]\n",
    "\n",
    "    # Plotting results with moving average\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c9a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n"
     ]
    }
   ],
   "source": [
    "# Initializing empty arrays\n",
    "x_timestep_total = []\n",
    "y_timestep_total = []\n",
    "x_episode_total = []\n",
    "y_episode_total = []\n",
    "mean_reward_total = []\n",
    "std_reward_total = []\n",
    "count = 0\n",
    "environment_name = \"BipedalWalker-v3\" # specifying environment name\n",
    "\n",
    "# Specifying parameters for grid search\n",
    "learning_rate = [0.00005, 0.0005, 0.005]\n",
    "gamma = [0.99, 0.95]\n",
    "batch_size = [32, 64, 128]\n",
    "\n",
    "# Neural network\n",
    "policy_kwargs = dict(activation_fn=th.nn.Tanh,\n",
    "                     net_arch=[dict(pi=[100], vf=[75])])\n",
    "\n",
    "\n",
    "# Looping over parameter values and training the models\n",
    "for i in learning_rate:\n",
    "    for j in gamma:\n",
    "        for k in batch_size:\n",
    "\n",
    "            print('Model',count)\n",
    "\n",
    "            env = gym.make(environment_name)\n",
    "\n",
    "            # Specifying log file environment\n",
    "            log_dir = \"logs/model\" + str(count) + \"/\"\n",
    "            env = Monitor(env, log_dir)\n",
    "\n",
    "            # Applying parameters to PPO model\n",
    "            model = PPO(\"MlpPolicy\", env, verbose=0,\n",
    "                        learning_rate = i,\n",
    "                        gamma = j,\n",
    "                        batch_size = k,\n",
    "                        policy_kwargs = policy_kwargs,\n",
    "                        seed = 42)\n",
    "\n",
    "            # Training for 400000 timesteps\n",
    "            model.learn(total_timesteps=400000)\n",
    "            \n",
    "            # Evaluating results on 1000 episodes\n",
    "            mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=1000, render=False)\n",
    "\n",
    "            # Extracting values for plotting\n",
    "            x_timestep, y_timestep = ts2xy(load_results(log_dir), 'timesteps')\n",
    "            x_episode, y_episode = ts2xy(load_results(log_dir), 'episodes')\n",
    "\n",
    "            plot_results(log_dir)\n",
    "\n",
    "            # Printing mean and std reward for each model\n",
    "            print('Mean reward', mean_reward)\n",
    "            print('Std dev. reward', std_reward)\n",
    "            \n",
    "            # Appending to array\n",
    "            x_timestep_total.append(x_timestep)\n",
    "            y_timestep_total.append(y_timestep)\n",
    "            x_episode_total.append(x_episode)\n",
    "            y_episode_total.append(y_episode)\n",
    "\n",
    "            mean_reward_total.append(mean_reward)\n",
    "            std_reward_total.append(std_reward)\n",
    "\n",
    "            print('----------------')\n",
    "\n",
    "            count +=1 \n",
    "            env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ebe00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Converting the arrays to numpy arrays for processing\n",
    "x_timestep_total = np.array(x_timestep_total)\n",
    "y_timestep_total = np.array(y_timestep_total)\n",
    "x_episode_total = np.array(x_episode_total)\n",
    "y_episode_total = np.array(y_episode_total)\n",
    "\n",
    "# Applying scaler so all models are plotted on the same scale\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Plotting results\n",
    "plt.figure(figsize=(8, 4))\n",
    "for i in range(len(x_timestep_total)):\n",
    "    x_timestep_total_norm = min_max_scaler.fit_transform(x_timestep_total[i].reshape(-1,1))\n",
    "    x_timestep_total_norm = x_timestep_total_norm * 400000\n",
    "    plt.plot(x_timestep_total_norm,y_timestep_total[i])\n",
    "    plt.title('Reward vs Timestep')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
